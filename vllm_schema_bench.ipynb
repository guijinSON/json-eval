{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# vLLM JSONSchemaBench + SchemaBench\n",
        "\n",
        "Run JSONSchemaBench (epfl-dlab/jsonschemabench) and SchemaBench (thunlp/SchemaReinforcementLearning) using vLLM, with evaluation logic taken from the repos."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Setup\n",
        "Install dependencies if needed. vLLM requires a compatible CUDA setup for GPU usage."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Optional: install dependencies\n",
        "# !pip install -q vllm transformers datasets jsonschema jsonlines json5 tqdm numpy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "import sys\n",
        "\n",
        "BENCH_ROOT = Path(\"/Users/songuijin/benchmarks\")\n",
        "JSONSCHEMA_REPO = BENCH_ROOT / \"jsonschemabench-epfl\"\n",
        "SCHEMABENCH_REPO = BENCH_ROOT / \"schemabench\"\n",
        "\n",
        "sys.path.append(str(JSONSCHEMA_REPO))\n",
        "sys.path.append(str(SCHEMABENCH_REPO))\n",
        "\n",
        "print(JSONSCHEMA_REPO)\n",
        "print(SCHEMABENCH_REPO)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Model configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "MODEL_ID = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
        "TENSOR_PARALLEL_SIZE = 1\n",
        "MAX_MODEL_LEN = None\n",
        "DTYPE = \"auto\"\n",
        "GPU_MEMORY_UTILIZATION = 0.90\n",
        "\n",
        "TEMPERATURE = 0.0\n",
        "TOP_P = 0.95\n",
        "MAX_NEW_TOKENS = 512"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import time\n",
        "from vllm import LLM, SamplingParams\n",
        "\n",
        "llm_kwargs = {\n",
        "    \"model\": MODEL_ID,\n",
        "    \"tensor_parallel_size\": TENSOR_PARALLEL_SIZE,\n",
        "    \"dtype\": DTYPE,\n",
        "    \"gpu_memory_utilization\": GPU_MEMORY_UTILIZATION,\n",
        "}\n",
        "if MAX_MODEL_LEN is not None:\n",
        "    llm_kwargs[\"max_model_len\"] = MAX_MODEL_LEN\n",
        "\n",
        "llm = LLM(**llm_kwargs)\n",
        "tokenizer = llm.get_tokenizer()\n",
        "\n",
        "sampling_params = SamplingParams(\n",
        "    temperature=TEMPERATURE,\n",
        "    top_p=TOP_P,\n",
        "    max_tokens=MAX_NEW_TOKENS,\n",
        ")\n",
        "\n",
        "def render_chat_messages(messages):\n",
        "    if hasattr(tokenizer, \"apply_chat_template\"):\n",
        "        return tokenizer.apply_chat_template(\n",
        "            messages, tokenize=False, add_generation_prompt=True\n",
        "        )\n",
        "    return \"\\n\".join([f\"{m['role']}: {m['content']}\" for m in messages]) + \"\\nassistant:\"\n",
        "\n",
        "def extract_json_text(text):\n",
        "    if \"```json\" in text:\n",
        "        return text.split(\"```json\", 1)[1].split(\"```\", 1)[0].strip()\n",
        "    return text.strip()\n",
        "\n",
        "def generate_one(messages, params=None):\n",
        "    prompt = render_chat_messages(messages)\n",
        "    start = time.perf_counter()\n",
        "    outputs = llm.generate([prompt], params or sampling_params)\n",
        "    elapsed = time.perf_counter() - start\n",
        "\n",
        "    request_output = outputs[0]\n",
        "    output = request_output.outputs[0]\n",
        "\n",
        "    text = output.text\n",
        "    generation = extract_json_text(text)\n",
        "    prompt_tokens = len(request_output.prompt_token_ids) if request_output.prompt_token_ids else 0\n",
        "    output_tokens = len(output.token_ids) if output.token_ids else 0\n",
        "\n",
        "    tpot_ms = (elapsed / max(output_tokens, 1)) * 1000 if output_tokens else None\n",
        "\n",
        "    return {\n",
        "        \"prompt\": prompt,\n",
        "        \"raw_text\": text,\n",
        "        \"json\": generation,\n",
        "        \"elapsed\": elapsed,\n",
        "        \"prompt_tokens\": prompt_tokens,\n",
        "        \"output_tokens\": output_tokens,\n",
        "        \"tpot_ms\": tpot_ms,\n",
        "    }"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## JSONSchemaBench (epfl-dlab/jsonschemabench)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from core.dataset import Dataset, DatasetConfig, DATASET_NAMES\n",
        "from core.messages import FEW_SHOTS_MESSAGES_FORMATTER\n",
        "from core.evaluator import evaluate\n",
        "from core.types import (\n",
        "    GenerationOutput,\n",
        "    TokenUsage,\n",
        "    PerfMetrics,\n",
        "    CompileStatus,\n",
        "    CompileStatusCode,\n",
        ")\n",
        "\n",
        "TASK = \"Kubernetes\"\n",
        "LIMIT = 5\n",
        "\n",
        "print(\"Available tasks:\", DATASET_NAMES)\n",
        "\n",
        "dataset = Dataset(DatasetConfig(TASK, limit=LIMIT))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "sample_messages, sample_schema = next(dataset.iter(FEW_SHOTS_MESSAGES_FORMATTER))\n",
        "print(render_chat_messages(sample_messages))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "outputs = []\n",
        "for messages, schema in dataset.iter(FEW_SHOTS_MESSAGES_FORMATTER):\n",
        "    result = generate_one(messages)\n",
        "    output = GenerationOutput(\n",
        "        task=TASK,\n",
        "        messages=messages,\n",
        "        generation=result[\"json\"],\n",
        "        schema=schema,\n",
        "    )\n",
        "    output.metadata.compile_status = CompileStatus(code=CompileStatusCode.OK)\n",
        "    output.token_usage = TokenUsage(\n",
        "        input_tokens=result[\"prompt_tokens\"],\n",
        "        output_tokens=result[\"output_tokens\"],\n",
        "    )\n",
        "    output.perf_metrics = PerfMetrics(\n",
        "        tgt=result[\"elapsed\"],\n",
        "        tpot=result[\"tpot_ms\"],\n",
        "    )\n",
        "    outputs.append(output)\n",
        "\n",
        "dc, ec, compliance, perf_metrics, output_tokens = evaluate(outputs)\n",
        "\n",
        "def metric_to_dict(metric):\n",
        "    return {\n",
        "        \"min\": metric.min,\n",
        "        \"max\": metric.max,\n",
        "        \"median\": metric.median,\n",
        "        \"std\": metric.std,\n",
        "    }\n",
        "\n",
        "print(\"Declared coverage\", metric_to_dict(dc))\n",
        "print(\"Empirical coverage\", metric_to_dict(ec))\n",
        "print(\"Compliance\", metric_to_dict(compliance))\n",
        "print(\"TTFT\", metric_to_dict(perf_metrics.ttft))\n",
        "print(\"TPOT\", metric_to_dict(perf_metrics.tpot))\n",
        "print(\"TGT\", metric_to_dict(perf_metrics.tgt))\n",
        "print(\"GCT\", metric_to_dict(perf_metrics.gct))\n",
        "print(\"Output tokens\", metric_to_dict(output_tokens))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## SchemaBench (thunlp/SchemaReinforcementLearning)\n",
        "SchemaBench requires data files in `schemabench/data`. Download them from the repo README and place them in the correct folders before running."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from schemabench.bench.union import UnionSyntaxBench\n",
        "\n",
        "SCHEMABENCH_DATA = SCHEMABENCH_REPO / \"schemabench\" / \"data\"\n",
        "if not SCHEMABENCH_DATA.exists():\n",
        "    print(\"SchemaBench data not found. Download and place data into:\", SCHEMABENCH_DATA)\n",
        "\n",
        "BENCH_CATEGORY = \"schema\"  # all, schema, reasoning, complex, custom, escape, math500, mmlu, arc, gsm8k\n",
        "SUBSET = True\n",
        "N_SHOTS = 3\n",
        "MAX_SAMPLES = 5\n",
        "\n",
        "bench = UnionSyntaxBench(n_shots=N_SHOTS, subset=SUBSET, test_category=BENCH_CATEGORY)\n",
        "\n",
        "sample_item = next(iter(bench))\n",
        "sample_messages = sample_item.get_prompt()\n",
        "print(render_chat_messages(sample_messages))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import asyncio\n",
        "import itertools\n",
        "\n",
        "async def run_schemabench(bench, max_samples):\n",
        "    results = []\n",
        "    for benchitem in itertools.islice(bench, max_samples):\n",
        "        messages = benchitem.get_prompt()\n",
        "        result = await asyncio.to_thread(generate_one, messages)\n",
        "        pred = result[\"json\"]\n",
        "        error = None\n",
        "        try:\n",
        "            score = await benchitem.validate(pred)\n",
        "        except Exception as e:\n",
        "            score = False\n",
        "            error = f\"{type(e).__name__}: {e}\"\n",
        "        results.append({\n",
        "            \"dataset\": benchitem.question.dataset,\n",
        "            \"question\": benchitem.question.query,\n",
        "            \"schema\": benchitem.question.validate_schema,\n",
        "            \"answer\": pred,\n",
        "            \"score\": score,\n",
        "            \"error\": error,\n",
        "        })\n",
        "    return results\n",
        "\n",
        "results = await run_schemabench(bench, MAX_SAMPLES)\n",
        "accuracy = sum(1 for r in results if r[\"score\"]) / max(len(results), 1)\n",
        "\n",
        "print(\"SchemaBench accuracy\", accuracy)\n",
        "results[:2]"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
